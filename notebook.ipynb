{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e9c9a",
   "metadata": {},
   "source": [
    "# 📓 Brick E1 – Cloud Object Storage & Cloud-Native Formats\n",
    "\n",
    "**Part of Modern GIS Bricks**  \n",
    "*Learn to upload local GIS files, convert them to cloud-native formats, and write them back to object storage.*\n",
    "\n",
    "**Goals**  \n",
    "1. Upload GeoTIFF & Shapefile → S3/MinIO  \n",
    "2. Convert:\n",
    "   - GeoTIFF → COG\n",
    "   - Shapefile → GeoParquet  \n",
    "3. Write outputs back to cloud storage  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4622cf",
   "metadata": {},
   "source": [
    "## Lesson 1: Configuring your S3 environment\n",
    "\n",
    "We’ll read and write from S3.  You need to set your AWS credentials and bucket name before we can proceed.\n",
    "\n",
    "**Exercise:**  \n",
    "Fill in your AWS keys _or_ ensure they’re set in your environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Credentials\n",
    "import os\n",
    "from obstore.store import S3Store\n",
    "\n",
    "# ── Exercise: provide your AWS credentials (or ensure env-vars are set) ──────\n",
    "# TODO: remove hard-coded values and rely on Gitpod env-vars instead\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]     = \"YOUR_KEY_ID\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR_ACCESS_KEY\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"]    = \"us-east-1\"            # Experiment: try another region\n",
    "\n",
    "# Exercise: set your bucket name\n",
    "bucket = \"modern-gis\"          # TODO\n",
    "\n",
    "# Instantiate the store\n",
    "store = S3Store(bucket=bucket)\n",
    "print(\"✅ Connected to S3 bucket:\", bucket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c900dc",
   "metadata": {},
   "source": [
    "## 2. Upload Local Files to S3\n",
    "\n",
    "We’ll push these local files into your S3 bucket:\n",
    "\n",
    "- `data/esa_world_cover.tif`  \n",
    "- `data/parks.gpkg`\n",
    "\n",
    "Destination: `my-s3-bucket/e1-input/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Cell: Upload local sample files to S3 via obstore\n",
    "local_raster  = \"data/esa_world_cover.tif\"\n",
    "local_geopackage = \"data/parks.gpkg\"\n",
    "bucket        = \"modern-gis\"\n",
    "prefix = \"e1-input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46490e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put(\"e1-input/esa_world_cover.tif\", open(local_raster, \"rb\"))\n",
    "print(f\"Uploaded {local_raster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5855be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put(\"e1-input/parks.gpkg\", open(local_geopackage, \"rb\"))\n",
    "print(f\"Uploaded {local_geopackage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997bb15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. COG Best Practices\n",
    "\n",
    "A Cloud-Optimized GeoTIFF (COG) is a valid GeoTIFF laid out for fast HTTP range requests. Key recommendations:\n",
    "\n",
    "- **Internal tiling**: 256×256 or 512×512 blocks  \n",
    "- **Overviews**: generate downsampled levels (smallest ≲256 px)  \n",
    "- **Compression**: DEFLATE or LZW with PREDICTOR=2 (ints) or 3 (floats)  \n",
    "- **NoData**: explicitly set (e.g. `NaN` for floats, max-neg for ints)  \n",
    "- **Projection**: use a known EPSG code, WKT2 format  \n",
    "- **Web-optimized** (optional): align tiles & overviews to Web Mercator 256 px grid \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65397631",
   "metadata": {},
   "source": [
    "## Part 3.1: Fetching a Remote GeoTIFF into VSIMEM\n",
    "\n",
    "We’ll pull down the GeoTIFF bytes from S3 and wrap them in a Rasterio `MemoryFile` (GDAL’s `/vsimem/`).\n",
    "\n",
    "**Exercise:**\n",
    "1. Fill in `in_key` (the path to your source TIFF in the bucket).  \n",
    "2. Wrap the bytes in `io.BytesIO` so `MemoryFile` will accept them.  \n",
    "3. Print out `src_ds.profile` to inspect the source metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Read GeoTIFF as bytes → VSIMEM\n",
    "import io\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "# Exercise: path to your input file in S3\n",
    "prefix   = \"e1-input\"                             # TODO if you changed it\n",
    "in_key   = f\"{prefix}/esa_world_cover.tif\"        # TODO\n",
    "\n",
    "# 1️⃣ Fetch raw bytes from S3\n",
    "resp        = store.get(in_key)\n",
    "tiff_bytes  = resp.bytes()      # raw bytes pulled via obstore\n",
    "\n",
    "# 2️⃣ Wrap in BytesIO for MemoryFile\n",
    "data_buf = io.BytesIO(tiff_bytes)\n",
    "\n",
    "# 3️⃣ Open in VSIMEM and inspect\n",
    "with MemoryFile(file_or_bytes=data_buf) as src_mem:\n",
    "    with src_mem.open() as src_ds:\n",
    "        print(\"Source profile:\", src_ds.profile)  # Exercise: inspect this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ec4cb",
   "metadata": {},
   "source": [
    "## Part 3.2: Tuning the Output Profile\n",
    "\n",
    "A Cloud-Optimized GeoTIFF (COG) requires:\n",
    "\n",
    "* internal tiling (`tiled=True`, with block size settings)\n",
    "* an efficient compression algorithm and predictor\n",
    "* overviews (optionally added; here, handled automatically by `rio-cogeo` when needed)\n",
    "\n",
    "In this exercise, you’ll fetch a standard GeoTIFF from S3, modify its tiling and compression settings, convert it into a Cloud-Optimized GeoTIFF entirely in memory using `rio-cogeo`, and upload the optimized result back to S3—**without writing any intermediate files to disk.**\n",
    "\n",
    "**Key Concepts Covered:**\n",
    "\n",
    "1. **Remote I/O** via `obstore.get()`\n",
    "2. **In-memory file operations** using `rasterio.io.MemoryFile`\n",
    "3. **COG tuning** through the GDAL-compatible profile\n",
    "4. **Preset profiles** from `rio-cogeo` and customization with `cog_translate`\n",
    "5. **Zero-disk COG conversion** (optionally switchable to disk-based temp files)\n",
    "\n",
    "> **Exercise:**\n",
    ">\n",
    "> * Swap hardcoded paths with variables or environment variables.\n",
    "> * Set `in_key` and `out_key` to match your S3 layout.\n",
    "> * Experiment with different `blockxsize`, `compress`, and `predictor` settings to compare output size and read performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rio_cogeo.cogeo import cog_translate\n",
    "from rio_cogeo.profiles import cog_profiles\n",
    "from rasterio.io import MemoryFile\n",
    "import obstore as obs\n",
    "import io\n",
    "\n",
    "# 1️⃣ Configure store & keys\n",
    "# TODO: set your S3 keys\n",
    "in_key   = \"e1-input/esa_world_cover.tif\"\n",
    "out_key  = \"e1-output/esa_world_cover_cog.tif\"\n",
    "\n",
    "# ── 2️⃣ Fetch raw GeoTIFF bytes from S3 ──────────────────────────────────────\n",
    "resp       = store.get(in_key)\n",
    "tiff_bytes = resp.bytes()           # raw TIFF data\n",
    "\n",
    "# ── 3️⃣ Wrap in BytesIO so MemoryFile accepts it ─────────────────────────────\n",
    "data_buf = io.BytesIO(tiff_bytes)\n",
    "\n",
    "# ── 4️⃣ Open source in VSIMEM & build destination profile ───────────────────\n",
    "with MemoryFile(file_or_bytes=data_buf) as src_mem:\n",
    "    with src_mem.open() as src_ds:\n",
    "        # Copy source metadata & tweak for COG\n",
    "        dst_profile = src_ds.profile.copy()\n",
    "        dst_profile.update({\n",
    "            \"driver\":     \"GTiff\",            # must remain GTiff\n",
    "            \"tiled\":      True,               # required for internal tiles\n",
    "            \"blockxsize\": 512,                # TODO: try 256, 1024\n",
    "            \"blockysize\": 512,                # TODO\n",
    "            \"compress\":   \"DEFLATE\",          # TODO: try \"LZW\" or \"ZSTD\"\n",
    "            \"predictor\":  2,                  # 2 for ints, 3 for floats\n",
    "        })\n",
    "\n",
    "        # ── 5️⃣ Pick & tweak rio-cogeo profile ────────────────────────────────\n",
    "        profile = cog_profiles.get(\"deflate\")   # TODO: try \"lzw\" or \"zstd\"\n",
    "        profile[\"blockxsize\"] = dst_profile[\"blockxsize\"]\n",
    "        profile[\"blockysize\"] = dst_profile[\"blockysize\"]\n",
    "\n",
    "        # ── 6️⃣ Create COG entirely in memory ────────────────────────────────\n",
    "        with MemoryFile() as dst_mem:\n",
    "            with dst_mem.open(**dst_profile) as dst_ds:\n",
    "                cog_translate(\n",
    "                    src_ds.name,\n",
    "                    dst_ds.name,\n",
    "                    profile,\n",
    "                    nodata=src_ds.nodata,    # reuse source NoData\n",
    "                    in_memory=True           # TODO: try False to use temp files\n",
    "                )\n",
    "\n",
    "            # ── 7️⃣ Upload the in-memory COG back to S3 ───────────────────────\n",
    "            dst_mem.seek(0)\n",
    "            store.put(out_key, dst_mem.read())\n",
    "\n",
    "print(f\"✅ COG created & uploaded to s3://{bucket}/{out_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee4d16",
   "metadata": {},
   "source": [
    "# 4 GeoParquet Best Practices\n",
    "\n",
    "When writing GeoParquet:\n",
    "\n",
    "- **Version**: 1.1 with the `bbox` covering struct  \n",
    "- **Compression**: ZSTD  \n",
    "- **Row-group size**: 100 000–200 000 rows  \n",
    "- **Spatial partitions**: geohash, S2/H3, or admin boundaries  \n",
    "- **Hive partitions** (optional): e.g. `country_iso=*` to prune files server-side  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9cfad",
   "metadata": {},
   "source": [
    "## Part 4.1: Loading a GeoPackage into GeoPandas\n",
    "\n",
    "We’ll load your `/parks.gpkg` into memory via Fiona’s MemoryFile.  This avoids local files on disk.\n",
    "\n",
    "**Exercise:**  \n",
    "- Fill in the `gpkg_key` (path within your bucket).  \n",
    "- Then run and inspect `gdf.head()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load parks.gpkg into GeoDataFrame\n",
    "import io\n",
    "import geopandas as gpd\n",
    "from fiona.io import MemoryFile\n",
    "\n",
    "# Exercise: point to where your parks.gpkg lives\n",
    "prefix    = \"e1-input\"                 # TODO: adjust if needed\n",
    "gpkg_key  = f\"{prefix}/parks.gpkg\"     # TODO\n",
    "\n",
    "# 1️⃣ Fetch the file as raw bytes\n",
    "resp = store.get(gpkg_key)\n",
    "raw_bytes = resp.bytes()\n",
    "buf = io.BytesIO(raw_bytes)\n",
    "\n",
    "# 2️⃣ Load with Fiona’s MemoryFile\n",
    "with MemoryFile(file_or_bytes=buf) as memf:\n",
    "    with memf.open() as src:\n",
    "        gdf = gpd.GeoDataFrame.from_features(src, crs=src.crs)\n",
    "\n",
    "# Inspect\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fa1cd",
   "metadata": {},
   "source": [
    "## Part 4.2: Ensuring the right CRS\n",
    "\n",
    "Spatial indexing & Hilbert codes require **longitude/latitude** (EPSG:4326).  We’ll reproject if needed.\n",
    "\n",
    "**Exercise:**  \n",
    "- Verify `gdf.crs`  \n",
    "- Fill in the `if` so it reprojects to EPSG:4326.  \n",
    "- Try plotting `gdf.geometry.boundary` to see your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Reproject to lon/lat if necessary\n",
    "\n",
    "# TODO: ensure gdf ends up in EPSG:4326\n",
    "if gdf.crs.to_epsg() != 4326:\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "print(\"Reprojected CRS:\", gdf.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275f239",
   "metadata": {},
   "source": [
    "## Part 4.3: Sampling & Quadtree construction\n",
    "\n",
    "Rather than simple grids, we’ll build a **quadtree** over a random sample of your data’s centroids.\n",
    "\n",
    "**Exercise:**  \n",
    "- Adjust `SAMPLE_SIZE` to sample more or fewer points.  \n",
    "- Adjust `MAX_PTS_PER_CELL` to control leaf subdivision.  \n",
    "- Read the code in `quadtree_partition` and add a comment explaining how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build quadtree partitions\n",
    "import random\n",
    "\n",
    "# 1️⃣ Compute representative points\n",
    "pts = list(gdf.geometry.representative_point())\n",
    "\n",
    "# 2️⃣ Sample them\n",
    "SAMPLE_SIZE         = 5000     # TODO: experiment with 1000, 10000\n",
    "MAX_PTS_PER_CELL    = 200      # TODO: experiment with 50, 500\n",
    "\n",
    "if len(pts) > SAMPLE_SIZE:\n",
    "    sample_pts = random.sample(pts, SAMPLE_SIZE)\n",
    "else:\n",
    "    sample_pts = pts\n",
    "\n",
    "# 3️⃣ Quadtree function (read & annotate)\n",
    "def quadtree_partition(samples, bounds, max_pts):\n",
    "    cells = []\n",
    "    def subdivide(bbox, pts):\n",
    "        # If below threshold, stop splitting\n",
    "        if len(pts) <= max_pts:\n",
    "            cells.append(bbox)\n",
    "            return\n",
    "        minx, miny, maxx, maxy = bbox\n",
    "        midx, midy = (minx+maxx)/2, (miny+maxy)/2\n",
    "        # Define 4 quadrants\n",
    "        quadrants = [\n",
    "            (minx, miny, midx, midy),\n",
    "            (midx, miny, maxx, midy),\n",
    "            (minx, midy, midx, maxy),\n",
    "            (midx, midy, maxx, maxy),\n",
    "        ]\n",
    "        # Subdivide each quadrant that has sample points\n",
    "        for q in quadrants:\n",
    "            q_pts = [p for p in pts if q[0] <= p.x < q[2] and q[1] <= p.y < q[3]]\n",
    "            if q_pts:\n",
    "                subdivide(q, q_pts)\n",
    "    subdivide(bounds, sample_pts)\n",
    "    return cells\n",
    "\n",
    "# 4️⃣ Run it\n",
    "bounds = tuple(gdf.total_bounds)   # (minx, miny, maxx, maxy)\n",
    "cells  = quadtree_partition(sample_pts, bounds, MAX_PTS_PER_CELL)\n",
    "\n",
    "print(f\"Generated {len(cells)} quadtree cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e1cf7",
   "metadata": {},
   "source": [
    "## Part 4.4 Compute spatial key & sort your GeoDataFrame  \n",
    "Generate a Hilbert/geohash code and sort by it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one point per polygon and encode\n",
    "gdf[\"hcode\"] = (\n",
    "  gdf.geometry\n",
    "     .representative_point()\n",
    "     .apply(lambda pt: gh.encode(pt.x, pt.y, precision))\n",
    ")\n",
    "# sort by that key\n",
    "gdf_sorted = gdf.sort_values(\"hcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730fb8c",
   "metadata": {},
   "source": [
    "## Part 4.5: Writing a Single GeoParquet to S3\n",
    "\n",
    "In this step, you’ll convert a GeoDataFrame into a single GeoParquet file and upload it directly to S3—all in memory, with no disk writes. The output uses efficient compression (`ZSTD`), includes bounding box metadata, and sets a custom `row_group_size` for optimized reads in analytical workflows.\n",
    "\n",
    "**Key Concepts Covered:**\n",
    "\n",
    "* In-memory file creation with `io.BytesIO`\n",
    "* GeoParquet writing using PyArrow\n",
    "* Efficient S3 upload with `obstore.put()`\n",
    "* Configurable geometry encoding and row group tuning for performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcc846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Part 1: Write a single GeoParquet file to S3 ────────────────────────────────\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import io\n",
    "\n",
    "# 1️⃣ Build the GeoParquet into a bytes buffer\n",
    "buf = io.BytesIO()\n",
    "gdf_sorted.to_parquet(\n",
    "    buf,\n",
    "    index=False,\n",
    "    compression=\"ZSTD\",\n",
    "    schema_version=\"1.1.0\",\n",
    "    geometry_encoding=\"WKB\",     # or \"geoarrow\" if you want the Arrow extension type\n",
    "    write_covering_bbox=True,\n",
    "    engine=\"pyarrow\",\n",
    "    row_group_size=150_000\n",
    ")\n",
    "parquet_bytes = buf.getvalue()\n",
    "\n",
    "pq_key = \"e1-output/parks.parquet\"\n",
    "resp = obs.put(store, pq_key, parquet_bytes)\n",
    "print(resp)\n",
    "print(\"✅ GeoParquet written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74387860",
   "metadata": {},
   "source": [
    "## Part 4.6: Writing Partitioned GeoParquet Files to S3\n",
    "\n",
    "This step splits a GeoDataFrame by a quadtree-based `partition` column and writes each group as a separate GeoParquet file to S3. Each partition is serialized in-memory using `BytesIO`, compressed with `ZSTD`, and saved with optimized row groups for scalable reads.\n",
    "\n",
    "**Key Concepts Covered:**\n",
    "\n",
    "* Spatial partitioning using quadtree bins\n",
    "* Memory-based Parquet export per partition\n",
    "* Organized S3 layout (`cell=ID/parks.parquet`) for easy querying and retrieval\n",
    "ioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ab2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import geopandas as gpd\n",
    "import obstore as obs\n",
    "from obstore.store import S3Store\n",
    "\n",
    "# assume gdf_sorted already has a “partition” column from your quadtree step\n",
    "base_prefix = \"e1-output/parks_quadtree\"\n",
    "\n",
    "# ── Loop over each partition and write via GeoPandas + BytesIO ────────────────\n",
    "for pid, grp in gdf_sorted.groupby(\"partition\"):\n",
    "    # 1️⃣ Serialize this subset as GeoParquet into memory\n",
    "    buf = io.BytesIO()\n",
    "    grp.to_parquet(\n",
    "        buf,\n",
    "        index=False,\n",
    "        compression=\"ZSTD\",\n",
    "        schema_version=\"1.1.0\",\n",
    "        geometry_encoding=\"WKB\",\n",
    "        write_covering_bbox=True,\n",
    "        engine=\"pyarrow\",\n",
    "        row_group_size=150_000\n",
    "    )\n",
    "    parquet_bytes = buf.getvalue()\n",
    "    buf.close()\n",
    "\n",
    "    # 2️⃣ Upload the in-memory Parquet to S3\n",
    "    key = f\"{base_prefix}/cell={int(pid)}/parks.parquet\"\n",
    "    resp = obs.put(store, key, parquet_bytes)\n",
    "    print(f\"✅ Partition {pid} → s3://{bucket}/{key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062294fc",
   "metadata": {},
   "source": [
    "## Part 4.7: Visualizing your partitions\n",
    "\n",
    "Finally, let’s see how your quadtree cells align with the data.  We’ll plot the representative points and over-draw all cell rectangles.\n",
    "\n",
    "_No code to fill in here—just run and inspect!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "pts = gdf.geometry.representative_point()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter([p.x for p in pts], [p.y for p in pts], s=10)\n",
    "\n",
    "for (minx, miny, maxx, maxy) in cells:\n",
    "    ax.add_patch(Rectangle((minx,miny), maxx-minx, maxy-miny, fill=False))\n",
    "\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Quadtree Partitions & Representative Points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5272417",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5. (Optional) Preview Files from S3\n",
    "\n",
    "List and inspect the generated COG and GeoParquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c349c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Read the newly created remote datasets directly\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# 1️⃣ Read GeoParquet from S3 (via fsspec/S3)\n",
    "pq_s3_path = f\"s3://{bucket}/e1-output/parks.parquet\"\n",
    "gdf = gpd.read_parquet(\n",
    "    pq_s3_path,\n",
    "    storage_options={\"anon\": False}\n",
    ")\n",
    "print(\"✅ GeoParquet rows:\", len(gdf))\n",
    "\n",
    "# 2️⃣ Open the COG via GDAL’s VSIS3 driver\n",
    "cog_s3_path = f\"/vsis3/{bucket}/e1-input/e1-output/esa_world_cover_cog.tif\"\n",
    "with rasterio.open(cog_s3_path) as src:\n",
    "    print(\"✅ COG bounds:\", src.bounds)\n",
    "    print(\"✅ COG overview levels:\", src.overviews(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1b479",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "- ✅ Uploaded raw files for testing to S3 and local GitHub submission  \n",
    "- ✅ Converted to COG & GeoParquet using best practices  \n",
    "- Files stored under `my-minio-bucket/e1-output/`  \n",
    "\n",
    "**Next:** Brick E2 – Iceberg partitioning & time-travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Download the parquet locally & save minimal COG metadata for tests\n",
    "\n",
    "import os\n",
    "import json\n",
    "import obstore as obs\n",
    "from obstore.store import S3Store\n",
    "from rio_cogeo.cogeo import cog_info\n",
    "\n",
    "# Setup\n",
    "out_dir = \"tests/outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "cog_key = \"e1-input/e1-output/esa_world_cover_cog.tif\"\n",
    "cog_uri = f\"/vsis3/{bucket}/{cog_key}\"\n",
    "info    = cog_info(cog_uri)\n",
    "\n",
    "bbox = info[\"GEO\"].BoundingBox   # (minx, miny, maxx, maxy)\n",
    "blockx, blocky = info[\"IFD\"][0].Blocksize\n",
    "compression = info[\"Compression\"]\n",
    "overviews = [ifd.Decimation for ifd in info[\"IFD\"][1:]]\n",
    "\n",
    "# ── Build the minimal metadata dict ───────────────────────────────────────────\n",
    "meta = {\n",
    "    \"bounds\": list(bbox),\n",
    "    \"tile_size\": {\"x\": blockx, \"y\": blocky},\n",
    "    \"compression\": compression,\n",
    "    \"overviews\": overviews,\n",
    "}\n",
    "\n",
    "meta_path = os.path.join(out_dir, \"esa_world_cover_cog_meta.json\")\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"✅ Wrote COG metadata to\", meta_path)\n",
    "\n",
    "pq_key = \"e1-output/parks.parquet\"\n",
    "pq_local = os.path.join(out_dir, \"parks.parquet\")\n",
    "\n",
    "# fetch and write to local file\n",
    "resp = obs.get(store, pq_key)\n",
    "with open(pq_local, \"wb\") as f:\n",
    "    f.write(resp.bytes())\n",
    "\n",
    "print(\"✅ Downloaded GeoParquet to\", pq_local)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
